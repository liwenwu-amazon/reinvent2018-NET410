# KOPS Demo

## Objective

### Create kops cluster

```
* In public subnet
* One master node
* Two worker nodes across two availability zones
```

### Deploy application across the cluster

```
* Deploy busybox application
* Deploy hello-world application
```

### Packet flow for 3 communications pattern:

```
* pod-to-pod communication
* pod-to-service communication
* external-to-interal communication
```

### Understand concepts and limitations:

```
* Pods
* Service (types: Cluster-IP, NodePort, LoadBalance, Ingress)
* Network architecture (interfaces, bridge, route table, ip address)
```

## Getting Started

### For this workshop activity, we are using CloudFormation template to configure "Getting Started".

### Set up environment:

#### Access KOPS controller EC2 instance

```
ssh -i <path_to_your_ssh_private_kye.pem> ubuntu@<ec2-instance-public-ip>
```

#### Configure KOPS controller for AWS access:

```
* Configure AWS CLI:

$ aws configure
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: us-west-2
Default output format [None]: json

```

#### IAM user permission:

```
* In order to create cluster with AWS, IAM user requires following permissions:

* AmazonEC2FullAccess
* AmazonS3FullAccess
* IAMFullAccess
* AmazonVPCFullAccess
* AmazonRoute53FullAccess

* You can add permissions to your existing users or you can create a new user. For this activity, we will create a new IAM user: 'kops'

aws iam create-group --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops
aws iam create-user --user-name kops
aws iam add-user-to-group --user-name kops --group-name kops
aws iam create-access-key --user-name kops

```

#### DNS Setup:

```
* You can configure DNS to suite your needs. Kops 1.6.2 or later allows you to create a gossip-based cluster. If you are using gossip-based cluster, you don't need to configure DNS. When using gossip-based cluster, your cluster name should end with .k8s.local

* For this activity we are going to create gossip-based cluster. Refer to Appendix B for DNS configuaration when using non gossip-based cluster
```

#### Cluster State Storage:

* Kops requires to store the state and represenation of the cluster. Create a dedicated S3 bucket that kops can use for this purpose

```
* Note: S3 requires --create-bucket-configuration LocationConstraint=<region> for regions other than us-east-1.
aws s3api create-bucket --bucket net410-kops-cluster-state-store-<aws-account-id> --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2

* Note: It is STRONGLY recommend to enable versioning on your S3 bucket. Versioning allows you to revert or recover a previous state store.
aws s3api put-bucket-versioning --bucket net410-kops-cluster-state-store-<aws-account-id>  --versioning-configuration Status=Enabled

* export KOPS_STATE_STORE
KOPS_STATE_STORE=s3://net410-kops-cluster-state-store-<aws-account-id>

* Kops will use this location by default. We suggest putting this in your bash profile or similar.
```

#### Create SSH key-pair:

* For this activity, we will create SSH key-pair to access the cluster. You can use your own key-pair.

```
aws ec2 create-key-pair --key-name net410_clusterAdmin |jq -r '.KeyMaterialâ€™ >net410_admin.pem
ssh-keygen -y -f net410_admin.pem >net410_clusterAdmin.pub
```

#### Create kops cluster:

```
kops create cluster \
  --name net410-kops-cluster.k8s.local \
  --master-count 1 \
  --master-size "t2.small" \
  --node-count 1 \
  --node-size "t2.small" \
  --network-cidr "10.0.0.0/16" \
  --zones "us-west-2a,us-west-2b,us-west-2c" \
  --ssh-public-key "~/.ssh/k8s_admin.pub" \
  --networking "kubenet"

* Create command only creates configuraiton for the cluster. It does not create any AWS cloud resources
* This allows you to review your cluster configure before creating resources:

kops get cluster
kops edit cluster net410-kops-cluster.k8s.local
kops edit ig --name net410-kops-cluster.k8s.local nodes
kops edit ig --name net410-kops-cluster.k8s.local master-us-west-2a

* Create AWS cloud resources after reviewing cluster configuration

kops update net410-kops-cluster.k8s.local --yes
```

## Deploy busybox application on the cluster

### Cluster information:

```
* Cluster that was created using CloudFormation consists of 1 master node and 2 worker nodes.

$: kops get cluster
NAME                CLOUD   ZONES
net410-kops-cluster.k8s.local   aws us-west-2a,us-west-2b,us-west-2c
$: kops validate cluster
Using cluster from kubectl context: net410-kops-cluster.k8s.local

Validating cluster net410-kops-cluster.k8s.local

INSTANCE GROUPS
NAME            ROLE    MACHINETYPE MIN MAX SUBNETS
master-us-west-2a   Master  t2.small    1   1   us-west-2a
nodes           Node    t2.small    2   2   us-west-2a,us-west-2b,us-west-2c

NODE STATUS
NAME                        ROLE    READY
ip-10-1-1-28.us-west-2.compute.internal     master  True
ip-10-1-2-179.us-west-2.compute.internal    node    True
ip-10-1-3-165.us-west-2.compute.internal    node    True

Your cluster net410-kops-cluster.k8s.local is ready
$:

* Cluster node information:

$: kubectl get nodes -o wide
NAME                                       STATUS   ROLES    AGE   VERSION   EXTERNAL-IP      OS-IMAGE                      KERNEL-VERSION   CONTAINER-RUNTIME
ip-10-1-1-28.us-west-2.compute.internal    Ready    master   2d    v1.10.6   34.218.224.163   Debian GNU/Linux 8 (jessie)   4.4.121-k8s      docker://17.3.2
ip-10-1-2-179.us-west-2.compute.internal   Ready    node     2d    v1.10.6   18.236.63.230    Debian GNU/Linux 8 (jessie)   4.4.121-k8s      docker://17.3.2
ip-10-1-3-165.us-west-2.compute.internal   Ready    node     2d    v1.10.6   18.237.189.223   Debian GNU/Linux 8 (jessie)   4.4.121-k8s      docker://17.3.2
$:

* For cluster operations, it did create pods in kube-system namespace:

$: kubectl get pods -o wide -n kube-system
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE
dns-controller-6d6b7f78b-5cznb                                    1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
etcd-server-events-ip-10-1-1-28.us-west-2.compute.internal        1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
etcd-server-ip-10-1-1-28.us-west-2.compute.internal               1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-apiserver-ip-10-1-1-28.us-west-2.compute.internal            1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-controller-manager-ip-10-1-1-28.us-west-2.compute.internal   1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-dns-5fbcb4d67b-hr4vr                                         3/3     Running   0          17h   100.65.129.4   ip-10-1-2-179.us-west-2.compute.internal
kube-dns-5fbcb4d67b-s26x8                                         3/3     Running   0          2d    100.65.130.2   ip-10-1-3-165.us-west-2.compute.internal
kube-dns-autoscaler-6874c546dd-k2twt                              1/1     Running   0          2d    100.65.129.2   ip-10-1-2-179.us-west-2.compute.internal
kube-proxy-ip-10-1-1-28.us-west-2.compute.internal                1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-proxy-ip-10-1-2-179.us-west-2.compute.internal               1/1     Running   0          2d    10.1.2.179     ip-10-1-2-179.us-west-2.compute.internal
kube-proxy-ip-10-1-3-165.us-west-2.compute.internal               1/1     Running   0          2d    10.1.3.165     ip-10-1-3-165.us-west-2.compute.internal
kube-scheduler-ip-10-1-1-28.us-west-2.compute.internal            1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kubernetes-dashboard-7b9c7bc8c9-ttc7z                             1/1     Running   0          2d    100.65.129.3   ip-10-1-2-179.us-west-2.compute.internal
$:

* We yet need to deploy application, hence, you won't see any pods running in default name space:

$: kubectl get pods -o wide
No resources found.
$: kubectl get pods -n default -o wide
No resources found.
$:

* View pods in all namepsace by specifying --all-namespaces:

$: kubectl get pods -o wide --all-namespaces
NAMESPACE     NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE
kube-system   dns-controller-6d6b7f78b-5cznb                                    1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   etcd-server-events-ip-10-1-1-28.us-west-2.compute.internal        1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   etcd-server-ip-10-1-1-28.us-west-2.compute.internal               1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   kube-apiserver-ip-10-1-1-28.us-west-2.compute.internal            1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   kube-controller-manager-ip-10-1-1-28.us-west-2.compute.internal   1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   kube-dns-5fbcb4d67b-hr4vr                                         3/3     Running   0          18h   100.65.129.4   ip-10-1-2-179.us-west-2.compute.internal
kube-system   kube-dns-5fbcb4d67b-s26x8                                         3/3     Running   0          2d    100.65.130.2   ip-10-1-3-165.us-west-2.compute.internal
kube-system   kube-dns-autoscaler-6874c546dd-k2twt                              1/1     Running   0          2d    100.65.129.2   ip-10-1-2-179.us-west-2.compute.internal
kube-system   kube-proxy-ip-10-1-1-28.us-west-2.compute.internal                1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   kube-proxy-ip-10-1-2-179.us-west-2.compute.internal               1/1     Running   0          2d    10.1.2.179     ip-10-1-2-179.us-west-2.compute.internal
kube-system   kube-proxy-ip-10-1-3-165.us-west-2.compute.internal               1/1     Running   0          2d    10.1.3.165     ip-10-1-3-165.us-west-2.compute.internal
kube-system   kube-scheduler-ip-10-1-1-28.us-west-2.compute.internal            1/1     Running   0          2d    10.1.1.28      ip-10-1-1-28.us-west-2.compute.internal
kube-system   kubernetes-dashboard-7b9c7bc8c9-ttc7z                             1/1     Running   0          2d    100.65.129.3   ip-10-1-2-179.us-west-2.compute.internal
$:

```

* Access one of the nodes and check for networking details:

admin@ip-10-1-2-179:~$ ip link show |grep -i "state up"
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
4: cbr0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 9001 qdisc htb state UP mode DEFAULT group default qlen 1000
5: veth7fcda148@docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue master cbr0 state UP mode DEFAULT group default
6: veth607c472d@docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue master cbr0 state UP mode DEFAULT group default
7: vethff861804@docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue master cbr0 state UP mode DEFAULT group default
admin@ip-10-1-2-179:~$ ip addr show eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 06:97:df:a2:0f:f4 brd ff:ff:ff:ff:ff:ff
    inet 10.1.2.179/24 brd 10.1.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::497:dfff:fea2:ff4/64 scope link
       valid_lft forever preferred_lft forever
admin@ip-10-1-2-179:~$ ip addr show cbr0
4: cbr0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 9001 qdisc htb state UP group default qlen 1000
    link/ether 0a:58:64:41:81:01 brd ff:ff:ff:ff:ff:ff
    inet 100.65.129.1/24 scope global cbr0
       valid_lft forever preferred_lft forever
    inet6 fe80::dced:1dff:fe3d:6f03/64 scope link
       valid_lft forever preferred_lft forever
admin@ip-10-1-2-179:~$ sudo brctl show
bridge name bridge id       STP enabled interfaces
cbr0        8000.0a5864418101   no      veth607c472d
                                        veth7fcda148
                                        vethff861804
docker0     8000.02424e9c2262   no
admin@ip-10-1-2-179:~$ ip route show
default via 10.1.2.1 dev eth0
10.1.2.0/24 dev eth0  proto kernel  scope link  src 10.1.2.179
100.65.129.0/24 dev cbr0  proto kernel  scope link  src 100.65.129.1
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1
admin@ip-10-1-2-179:~$

* vethxx interfaces are for pods that are running kube-system name space:

$: kubectl get pods -o wide --all-namespaces |grep 10-1-2-179
kube-system   kube-dns-5fbcb4d67b-hr4vr                                         3/3     Running   0          18h   100.65.129.4   ip-10-1-2-179.us-west-2.compute.internal
kube-system   kube-dns-autoscaler-6874c546dd-k2twt                              1/1     Running   0          2d    100.65.129.2   ip-10-1-2-179.us-west-2.compute.internal
kube-system   kube-proxy-ip-10-1-2-179.us-west-2.compute.internal               1/1     Running   0          2d    10.1.2.179     ip-10-1-2-179.us-west-2.compute.internal
kube-system   kubernetes-dashboard-7b9c7bc8c9-ttc7z                             1/1     Running   0          2d    100.65.129.3   ip-10-1-2-179.us-west-2.compute.internal
$:

### Deploy application

* You can deploy application:
  * By directly passing arguments to kubectl cli
  * Define resource in a file (yaml or jason format; yaml preferred) and pass file as an argument to kubectl cli. Advantage of using file base approach is it allows to you keep track of what you have a launched and allows you make any changes.
  * For this workshop will deploy it using a configuraiton file. All configuration file are located under ~/kubernetes/kops/configFiles/

```
$: kubectl create -f busyboxDeployment.yaml
deployment.apps/net410-kops-busybox created
$:
```

* It will create two pods, one on each node:

```
$: kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP             NODE
net410-kops-busybox-55d8557b4d-6gxrj   1/1     Running   0          1m    100.65.130.9   ip-10-1-3-165.us-west-2.compute.internal
net410-kops-busybox-55d8557b4d-b8w5n   1/1     Running   0          1m    100.65.129.6   ip-10-1-2-179.us-west-2.compute.internal
$:
```

* Lets check the node again:

```
* You should see vethxx for busybox pod:

admin@ip-10-1-2-179:~$ sudo brctl show cbr0
bridge name bridge id       STP enabled interfaces
cbr0        8000.0a5864418101   no      veth607c472d
                                        veth7fcda148
                                        veth8175afaa
                                        vethff861804
admin@ip-10-1-2-179:~$

admin@ip-10-1-2-179:~$ ip addr show veth8175afaa
9: veth8175afaa@docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue master cbr0 state UP group default
    link/ether 02:62:8f:a3:5d:44 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::62:8fff:fea3:5d44/64 scope link
       valid_lft forever preferred_lft forever
admin@ip-10-1-2-179:~$
```

* Access the pod:
